\chapter[Código]{Código}
\label{Chap5}

\section{Creación de Spiders}
Una vez instalado Scrapy, en nuestro directorio escogido escribimos el siguiente comando para generar un nuevo proyecto de Scrapy.

\begin{verbatim}
	scrapy startproject miproyecto
\end{verbatim}

Nos creara un nuevo directorio con el siguiente contenido.

\begin{figure} [h!]
	\centering
	\includegraphics[width=0.3\textwidth]{fig/estructura_proyecto_scrapy.png}
	\caption[Estructura del proyecto recién creado]{Estructura del proyecto recién creado}
	\label{fig:ej11}
\end{figure}

Primero entraremos en el directorio recientemente creado y luego ejecuteremos el comando encargado de crear la Spider.

\begin{verbatim}
	cd miproyecto
	scrapy genspider mispider webausar.com
\end{verbatim}

En caso de no especificar el protocolo usado por la web Scrapy asumirá que usa HTTPS.\newline
\newline
Tras ejecutar el comando la Spider habrá sido generada dentro de la carpeta spiders.

\begin{figure} [h!]
	\centering
	\includegraphics[width=0.3\textwidth]{fig/primera_spider.png}
	\caption[Directorio de almacenamiento de las Spider]{Directorio de almacenamiento de las Spider}
	\label{fig:ej12}
\end{figure}

Una vez abierto el archivo vemos que dispone del siguiente código.

\begin{lstlisting}[language=Python, caption={Spider recién generada}]
	import scrapy
	
	
	class MispiderSpider(scrapy.Spider):
		name = "mispider"
		allowed_domains = ["webausar.com"]
		start_urls = ["https://webausar.com"]
	
		def parse(self, response):
			pass
\end{lstlisting}

Como podemos ver Scrapy usa una programación orientada a objetos, siendo cada Spider una clase representada dentro del proyecto.\newline
\newline
Analizando las variables definidas vemos las siguientes, name, nombre por el que debemos referenciar la Spider a la hora de ejecutarla; allowed\_domains, indica que dominios podemos visitar, negando la entrada a cualquier dominio que no este definido en ella, es importante no especificar protocolo, de esta manera funcionara para cualquier web ya sea HTTP como HTTPS que pertenezca a ese dominio, de lo contrario se limitara al protocolo indicado; start\_urls, URL inicial sobre la que se hará la request de petición de datos.\newline
\newline
El método parse es aquel al que se envía la respuesta obtenida de la web, para realizar el filtrado de la información, quedándote unicamente con la deseada. Este método es invocado automáticamente por la Spider, sin necesidad real de hacerlo tu manualmente. Puede ser un método recursivo en caso de así quererlo o incluso se pueden definir nuevas funciones parse (usando un nombre distinto) en caso de necesitarlas.

\subsection{Proceso de obtención de datos}
Para poder realizar es la extracción de los datos, primero debemos ir a la web deseada e inspeccionar su estructuración. Para ello como ejemplo vamos a usar la web de aemet.\newline

\begin{figure} [H]
	\centering
	\includegraphics[width=0.8\textwidth]{fig/code_aemet.png}
	\caption[URL de inicio para obtener los códigos de las estaciones de Aemet]{Web de Aemet para obtención de datos}
	\label{fig:ej13}
\end{figure}

Una vez encontrada la web deseada, accediendo mediante el F12 a la herramienta de inspección, buscamos el elemento representativo del dato deseado. En nuestro caso queremos obtener tanto el nombre como el código de la estación. Ambos se encuentran en el mismo elemento que forma la primera columna de la tabla.\newline

\begin{figure} [H]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/inspector.png}
	\caption[Inspector de webs de Chrome]{Inspector de webs}
	\label{fig:ej14}
\end{figure}

Como en este caso es posible filtrar fácilmente los datos, los obtendremos todos directamente, aunque lo más común sería obtener las filas primero para luego iterar por cada una de ellas. Para obtenerlos podemos hacerlo mediante el selector de XPath como con el de CSS.\newline
\newline
Ambos se pueden obtener fácilmente en la herramienta de inspección, una vez seleccionado el elemento deseado, hacemos click derecho sobre el, vamos al apartado copiar y en el nos mostrara la posibilidad de copiar ambos selectores. Es probable que el selector proporcionado no sea del todo lo que busquemos o se pueda simplificar, por lo que es recomendable comprobarlo manualmente.\newline

\begin{verbatim}
	rows = response.xpath('//div[@id="contenedor_tabla"]/table/tbody/tr/td/a')
	ó
	rows = response.css("div#contenedor_tabla tbody tr a")
\end{verbatim}

Esto nos devuelve una lista de objetos tipo Selector, cosa que nos permite conforme vamos iterando por cada elemento volver a usar un selector para filtrar unicamente los datos deseados. En nuestro caso.

\begin{verbatim}
	path = rows[i].xpath("@href").get()
	name = rows[i].xpath("./text()").get()
	ó
	path = rows[i].css("*::attr(href)").get()
	name = rows[i].css("*::text").get()
\end{verbatim}

De esta forma, mediante el uso de la función get(), pasamos de tener un objeto Selector a un String. El uso de get() sobre una lista devuelve el primer elemento, en caso de querer transformar toda la lista el método a usar es getall().\newline
\newline
Finalmente, como de la URL obtenida,

\begin{verbatim}
	'/es/eltiempo/observacion/ultimosdatos?k=nav&l=9263X&w=0&datos=det&f=precipitacion'
\end{verbatim}

solo nos interesa el código de la estación (parámetro l de la query), lo filtraremos.

\begin{verbatim}
	code = path.split('&')[1].split('=')[1]
\end{verbatim}

\subsection{Guardado de datos}
Scrapy almacena todos los datos en forma de múltiples diccionarios, tantos como webs usadas. Para acceder a esta información Scrapy nos proporciona dos alternativas, el uso de Items junto a ItemLoaders, siendo clases especificas de Scrapy o, mediante la palabra reservada yield de Python, que tiene una funcionalidad parecida a return, siendo esta la opción elegida debido a su fácil implementación.\newline
\newline
De esta forma escribiremos.

\begin{lstlisting}[language=Python, caption={Guardar datos}]
	yield {
		'estacion': name,
		'codigo': path.split('&')[1].split('=')[1],
	}
\end{lstlisting}

Actualmente si se ejecuta la Spider nos imprimiría los datos obtenidos por pantalla, aunque pueden ser almacenados en un fichero tanto CSV como JSON, a la hora de ejecutar la Spider añadiendo en el comando "-o nombre.csv ó -o nombre.json".\newline
\newline
Para un uso ligero de forma manual esa alternativa es más que suficiente, pero en nuestro caso, al querer ejecutarlas de forma automática mediante el uso de Runners, debemos implementar una variable llamada custom\_settings para cada una de las Spider. Esta permite, sin la necesidad de modificar el archivo settings.py, añadir configuraciones o dependencias independientes en las Spiders.

\begin{lstlisting}[language=Python, caption={Confugurar guardado en JSON}]
	custom_settings = {
		'FEEDS': {
			'JSONs/RawCode/codigos_aemet.json': {
				'format': 'json',
				'encoding': 'utf-8',
				'overwrite': True,
			}
		}
	}
\end{lstlisting}

Con esto indicamos que, en la ruta especificada, nos almacene un fichero JSON utf-8 y, que cada vez que se llame a esta Spider sobre-escriba el fichero anterior.

\subsection{Spider básica}
Una vez obtenemos los datos y los podemos almacenar, ya estaría nuestra Spider básica terminada.

\begin{lstlisting}[language=Python, caption={Spider de ejemplo}]
	import scrapy
	
	
	class AemetCodeSpider(scrapy.Spider):
		name = "aemet_code"
		allowed_domains = ["www.aemet.es"]
		start_urls = ["https://www.aemet.es/es/eltiempo/observacion/"
		"ultimosdatos?k=nav&w=0&datos=det&x=h24&f=precipitacion"]
		custom_settings = {
			'FEEDS': {
				'JSONs/RawCode/codigos_aemet.json': {
					'format': 'json',
					'encoding': 'utf-8',
					'overwrite': True,
				}
			}
		}
	
		def parse(self, response):
			rows = response.css("div#contenedor_tabla tbody tr a")
			
			for row in rows:
			path = row.xpath("@href").get()
			name = row.xpath("./text()").get()
			code = path.split('&')[1].split('=')[1]
			
			yield {
				'estacion': name,
				'codigo': code,
			}
\end{lstlisting}

\subsection{Método start\_requests()}
El método start\_requests() es llamado de forma automática al iniciar la Spider, siendo el encargado de hacer la llamada a la web indicada en start\_urls y, una vez obtenidos los datos llamar a la función parse, todo mediante un objeto Request de Scrapy, el cual devolverá un objeto tipo HTMLResponse. En caso de querer alterar el funcionamiento de la Spider este es el método a sobre-escribir.\newline
\newline
Como en nuestro caso queremos obtener los datos de todas las estaciones dentro de un mismo dominio, reescribiremos la función para que recorra el JSON con los códigos de estas y, hacer una llamada por estación con Request.\newline
\newline
El código quedaría de la siguiente manera.

\begin{lstlisting}[language=Python, caption={Sobre-escritura de start\_request()}]
	def start_requests(self):
		with open("JSONs/RawCode/codigos_aemet.json", encoding="utf-8") as f:
			data = json.load(f)
		for estacion in data:
			url = f'https://www.aemet.es/es/eltiempo/observacion'
			'/ultimosdatos?k=nav&l={estacion["codigo"]}&w=0&'
			'datos=det&x=&f=temperatura'
			yield scrapy.Request(url, self.parse)
\end{lstlisting}

Al definir la función de esta manera no es necesario declarar la variable start\_urls, por lo que siempre que necesitemos sobre-escribir la función, no usaremos la variable.

\subsection{Eliminar Log}
Cuando se verifique el correcto funcionamiento de la Spider es recomendable quitar el maximo numero de Log por pantalla posible, es por eso que, en el fichero settings.py escribiremos las siguientes lineas.

\begin{lstlisting}[language=Python, caption={Configurar LOG}]
	LOG_LEVEL = 'WARNING'
	LOG_ENABLED = False
\end{lstlisting}

\section{Spiders usadas}
Como se ha dicho anteriormente, para este proyecto se han creado cuatro proyectos de Scrapy, uno por cada web.

\subsection{Aemet}

\begin{lstlisting}[language=Python, caption={Aemet Code Spider}]
	import scrapy
	
	
	class AemetCodeSpider(scrapy.Spider):
		name = "aemet_code_spider"
		allowed_domains = ["www.aemet.es"]
		start_urls = ["https://www.aemet.es/es/eltiempo/observacion/"
		"ultimosdatos?k=nav&w=0&datos=det&x=h24&f=temperatura"]
		custom_settings = {
			'FEEDS': {
				'JSONs/RawCode/codigos_aemet.json': {
					'format': 'json',
					'encoding': 'utf-8',
					'overwrite': True,
				}
			}
		}
	
		def parse(self, response):
			rows = response.css("div#contenedor_tabla tbody tr a")
			
			for row in rows:
				path = row.xpath("@href").get()
				name = row.xpath("./text()").get()
				yield {
					'estacion': name,
					'codigo': path.split('&')[1].split('=')[1],
				}
\end{lstlisting}

Siendo esta la Spider usada como ejemplo no hay mucho más que comentar al respecto, se dedica a obtener los nombres y códigos de cada estación.

\begin{lstlisting}[language=Python, caption={Aemet Data Spider}]
	import json
	
	import scrapy
	
	
	class AemetDataSpider(scrapy.Spider):
		name = "aemet_data_spider"
		allowed_domains = ["www.aemet.es"]
		custom_settings = {
			'FEEDS': {
				'JSONs/RawData/datos_aemet.json': {
					'format': 'json',
					'encoding': 'utf-8',
					'overwrite': True,
				}
			}
		}
	
		def start_requests(self):
			with open("JSONs/RawCode/codigos_aemet.json", encoding="utf-8") as f:
				data = json.load(f)
			for estacion in data:
				url = f'https://www.aemet.es/es/eltiempo/observacion/'
				'ultimosdatos?k=nav&l={estacion["codigo"]}&w=0&'
				'datos=det&x=&f=temperatura'
				yield scrapy.Request(url, self.parse)
		
		def parse(self, response):
			latitud = response.css('abbr.latitude::text').get()
			longitud = response.css('abbr.longitude::text').get()
			estacion = response.css("a.separador_pestanhas").get()
			rows = response.css('tbody tr')
			
			datos = []
			for row in rows:
				dato = {
					'fecha y hora': row.xpath('./td[1]/text()').get() + ':00',
					'temperatura (C)': row.xpath('./td[2]/text()').get(),
					'humedad (%)': row.xpath('./td[10]/text()').get(),
					'precipitacion (mm)': row.xpath('./td[7]/text()').get(),
				}
				
				if dato['precipitacion (mm)'] != " ":
					datos.append(dato)
			
			yield {
				'coordenadas': latitud + ' | ' + longitud,
				'estacion': estacion.split('=')[3].split('&')[0],
				'datos': datos,
			}
\end{lstlisting}

Como vemos, la Spider de obtención de datos ya es un poco más compleja, necesitando sobre-escribir la función de start\_request(). También se puede observar como inicialmente todos los datos son almacenados en una lista antes de ser guardados, el motivo de esto es muy simple y, es que de no hacerlo solo recibiría el primer dato de entre todos los recogidos.\newline
\newline
A su vez dentro del bucle for se ha añadido un if para asegurarse de no guardar datos vacíos, pues puede darse el caso en el que la web aun no tenga el dato de precipitación a cierta hora pero si muestre esta franja horaria pues dispone de otros datos como pueden ser aquellos relacionados con el viento.\newline
\newline
Por otro lado, como las coordenadas son mostradas en la misma página que los datos, en vez de crear otra Spider específicamente para obtenerlos, se hace uso de esta con el fin de minimizar código y tiempo de ejecución.

\subsection{Chcantabrico}

\begin{lstlisting}[language=Python, caption={Chcantabrico Code Spider}]
	import scrapy
	
	
	class ChcantabricoCodeSpider(scrapy.Spider):
		name = "chcantabrico_code_spider"
		allowed_domains = ["www.chcantabrico.es"]
		start_urls = ["https://www.chcantabrico.es/nivel-de-los-rios"]
		custom_settings = {
			'FEEDS': {
				'JSONs/RawCode/codigos_chcantabrico.json': {
					'format': 'json',
					'encoding': 'utf-8',
					'overwrite': True,
				}
			}
		}
	
		def parse(self, response):
			rows = response.xpath('//table[@class="tablefixedheader niveles"]/tbody/tr')
			
			for row in rows:
				codigoBusqueda = row.css('td.codigo::text').get()
				limites = row.css('table.umbrales_gr td.datos::text').getall()
				paths = row.xpath('./td/a/@href').getall()
				estaciones = row.xpath('./td/a/text()').getall()
			
				for i in range(len(limites)):
					if limites[i] == 'No definido':
						limites[i] = None
				
				yield {
					'estacion': estaciones[-3],
					'codigo': paths[-1].split("=")[-1],
					'codigoSecundario': codigoBusqueda,
					'seguimiento': limites[0],
					'prealerta': limites[1],
					'alerta': limites[2],
				}
\end{lstlisting}

Esta puede ser la Spider de obtención de códigos más compleja. esto se debe en parte por la estructuración de la web, mostrando una tabla con otra tabla integrada por cada una de las filas, como por la cantidad de información que dispone.\newline
\newline
El primer problema fue como obtener las filas de la tabla principal, pues en muchos de los intentos realizados no solo tomaba estas filas, si no que tomaba aquellas que formaban parte de las tablas incluidas es ellas también. Finalmente, aunque con el Selector CSS no lo logré, mediante Xpath fue posible filtrarlas.\newline
\newline
Otra cosa a mencionar es que Chcantabrico proporciona dos códigos por estación, uno referenciando a la estación en si, siendo el usado para posteriormente obtener las coordenadas, aquel que llamo codigoSecundario o codigoBusqueda mientras que, el segundo hace referencia a los datos como tal. Puesto que no todas las filas disponen de la misma cantidad de enlaces, se obtienen todos sabiendo que siempre el ultimo de ellos dispone del código deseado.\newline
\newline
Finalmente, dentro de la tabla secundaria, esta es la única web que llega a proporcionar los valores de seguimiento, pre-alerta y alerta del río, siendo estos una buena base para empezar con las predicciones de inundación. En caso de no estar definido el valor simplemente se indicará como None.

\begin{lstlisting}[language=Python, caption={Chcantabrico Nivel Spider}]
	import io
	import json
	
	import pandas as pd
	import scrapy
	
	
	class ChcantabricoNivelSpider(scrapy.Spider):
		name = "chcantabrico_nivel_spider"
		allowed_domains = ["www.chcantabrico.es"]
		custom_settings = {
			'FEEDS': {
				'JSONs/RawData/datos_nivel_chcantabrico.json': {
					'format': 'json',
					'encoding': 'utf-8',
					'overwrite': True,
				}
			}
		}
	
		def start_requests(self):
			with open("JSONs/RawCode/codigos_chcantabrico.json", encoding="utf-8") as f:
				data = json.load(f)
			for estacion in data:
				params_nivel = {
					'p_p_id': 'GraficaEstacion_INSTANCE_wH0LL6jTUysu',
					'p_p_lifecycle': '2',
					'p_p_state': 'normal',
					'p_p_mode': 'view',
					'p_p_resource_id': 'downloadCsv',
					'p_p_cacheability': 'cacheLevelPage',
					'_GraficaEstacion_INSTANCE_wH0LL6jTUysu_cod_estacion': f'{estacion["codigo"]}',
					'_GraficaEstacion_INSTANCE_wH0LL6jTUysu_tipodato': 'nivel',
				}
				url = 'https://www.chcantabrico.es/evolucion-de-niveles'
				yield scrapy.FormRequest(url=url,
				method='GET',
				formdata=params_nivel,
				callback=self.parse,
				cb_kwargs={'estacion': estacion['codigo']}
				)
		
		def parse(self, response, estacion):
			if not response.text.startswith('-'):
				urlData = response.text
				rawData = pd.read_csv(io.StringIO(urlData), delimiter=';', encoding='utf-8', header=1)
				rawData.columns = ['fecha y hora', 'nivel (m)']
				parsedData = rawData.to_json(orient="records")
				
				yield {
					'estacion': estacion,
					'datos': json.loads(parsedData)
				}
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Chcantabrico Pluviometric Spider}]
	import io
	import json
	
	import pandas as pd
	import scrapy
	
	
	class ChcantabricoPluvioSpider(scrapy.Spider):
		name = "chcantabrico_pluvio_spider"
		allowed_domains = ["www.chcantabrico.es"]
		custom_settings = {
			'FEEDS': {
				'JSONs/RawData/datos_pluvio_chcantabrico.json': {
					'format': 'json',
					'encoding': 'utf-8',
					'overwrite': True,
				}
			}
		}
	
		def start_requests(self):
			with open("JSONs/RawCode/codigos_chcantabrico.json", encoding="utf-8") as f:
				data = json.load(f)
			for estacion in data:
				params_pluvio = {
					'p_p_id': 'GraficaEstacion_INSTANCE_ND81Xo17PIZ7',
					'p_p_lifecycle': '2',
					'p_p_state': 'normal',
					'p_p_mode': 'view',
					'p_p_resource_id': 'downloadCsvPluvio',
					'p_p_cacheability': 'cacheLevelPage',
					'_GraficaEstacion_INSTANCE_ND81Xo17PIZ7_cod_estacion': f'{estacion["codigo"]}',
					'_GraficaEstacion_INSTANCE_ND81Xo17PIZ7_tipodato': 'pluvio',
				}
				url = 'https://www.chcantabrico.es/precipitacion-acumulada'
				yield scrapy.FormRequest(url=url,
				method='GET',
				formdata=params_pluvio,
				callback=self.parse,
				cb_kwargs={'estacion': estacion['codigo']}
				)
		
		def parse(self, response, estacion):
			if not response.text.startswith('-'):
				urlData = response.text
				rawData = pd.read_csv(io.StringIO(urlData), delimiter=';', encoding='utf-8', header=1)
				rawData.columns = ['fecha y hora', 'nivel (m)']
				parsedData = rawData.to_json(orient="records")
				
				yield {
					'estacion': estacion,
					'datos': json.loads(parsedData)
				}
\end{lstlisting}

Chacantabrico muestra datos tanto del nivel del río como de la precipitación, aunque lo hace en dos direcciones distintas, haciendo necesario el uso de dos Spiders.\newline
\newline
A su vez, como Chcantabrico no muestra los datos por pantalla, incluyendo un botón sobre el que pulsar para obtenerlos descargando un fichero CSV, en vez de hacer una request básica mediante la clase Request, vamos ha hacer uso de FormRequest para hacer una llamada GET, pudiendo simular la llamada a un formulario y obtener los datos que este devuelve. De esta forma, pasandole los parámetros necesarios en el argumento formdata a la URL indicada, podemos obtener los datos sin la necesidad de ningún CSV, simulando en cierto modo una llamada mediante cURL. Cabe mencionar que, Request devuelve un HTMLResponse y que, la respuesta que obtenemos de estas llamadas no es codigo HTML, por lo que, aun en caso de que llegue a ser posible usar Request, es más correcto el uso de FormRequest devolviendo un FormResponse para este tipo de casos.\newline
\newline
El uso del argumento cb\_kwargs sirve para enviar un mayor numero de argumentos a la función parse() de los que normalmente recibe, es por ello que la función parse() recibe un tercer argumento que hemos decidido llamar estación. En este caso para poder enviar a cada conjunto de datos recibidos el código de la estación a la que pertenecen, pues dentro de la respuesta obtenida solo se proporciona el nombre de esta.\newline
\newline
Dentro de la función parse() lo primero que se hace es comprobar que realmente se ha recibido una respuesta correcta, pues, aunque todas la estaciones disponen de datos del nivel del río, no todas disponen los de precipitación, el problema viene cuando a estas estaciones se les piden los datos, ya que en vez de enviar un error 404 como seria esperado.

\begin{verbatim}
	-
	FECHA;VALOR(mm)
\end{verbatim}

Una alternativa para deshacerse de esta comprobación sería eliminando aquellas estaciones que no proporcionen datos o filtrandolas para no hacer la llamada directamente, aunque esto no solo nos resultaría más complejo, si no que nos crearía el problema de que cada cierto tiempo habría que comprobar si alguna estación ha empezado a proporcionar datos para incluirla nuevamente en la lista de estaciones a las cuales hacer llamada.\newline
\newline
Una vez hecha la comprobación, en caso de que no sea una respuesta vacía, obtenemos el texto que viene proporcionado con el siguiente formato.

\begin{verbatim}
	Ribera de Piquín
	FECHA;VALOR(m)
	03/08/2023 11:30:00;0.153
	03/08/2023 11:45:00;0.153
	03/08/2023 12:00:00;0.153
	...
\end{verbatim}

Ese texto, al tener formato CSV lo leemos mediante la función read\_csv() incluida en la librería pandas, indicamos el delimitador, la codificación y la linea que representa la cabecera, empezando de la 0, en este caso la 1, pues no nos interesa el nombre de la estación. Finalmente, como la función espera que se le pase una ruta a un fichero, lo que hacemos mediante io.StringIO() es crear un objeto con el que simular un fichero en memoria, pasando de disponer texto plano a un DataFrame de pandas.\newline
\newline
Como últimos pasos, cambiamos los nombres de las cabeceras a aquellos definidos de forma global para todas las webs y, convertimos el DataFrame en un fichero JSON con la función to\_json() indicando que el formato sea "records", esto implica que cada linea del DataFrame va a representar un objeto JSON.

\begin{lstlisting}[language=Python, caption={Chcantabrico Coordinates Spider}]
	import json
	
	import scrapy
	
	
	class ChcantabricoCoordSpider(scrapy.Spider):
		name = "chcantabrico_coord_spider"
		allowed_domains = ["ceh.cedex.es"]
		custom_settings = {
			'FEEDS': {
				'JSONs/RawData/coordenadas_chcantabrico.json': {
					'format': 'json',
					'encoding': 'utf-8',
					'overwrite': True,
				}
			}
		}
		
		def start_requests(self):
		with open("JSONs/RawCode/codigos_chcantabrico.json", encoding="utf-8") as f:
			data = json.load(f)
			for estacion in data:
				url = f'https://ceh.cedex.es/anuarioaforos/afo/estaf-datos.asp?'
				'indroea={estacion["codigoSecundario"]}'
				yield scrapy.Request(url, self.parse)
		
		def parse(self, response):
			longitud = response.css('p::text')[6].get().strip()
			latitud = response.css('p::text')[7].get().strip()
			estacion = response.css('font::text')[14].get().strip()
			
			yield {
				'coordenadas': f'Lat: {latitud} | Lon: {longitud}',
				'estacion': estacion,
			}
\end{lstlisting}

Aunque en la web misma se proporciona un mapa indicando la localización de cada estación, las coordenadas de esta no están disponibles para adquirir dentro de la web, es por eso que es necesario el uso de la web del centro de estudios hidrológicos para poder obtener las coordenadas. En esta página las podemos encontrar mediante el "codigoSecundario" anteriormente obtenido, desgraciadamente, no todas las estaciones incluidas en Chacantrabrico están listadas en esta página, siendo el mayor inconveniente para el correcto funcionamiento del apartado de predicción.\newline
\newline
Exceptuando el uso del "codigoSecundario" para referenciar estaciones, esta es una Spider muy simple la cual no dispone de nada que no haya sido anteriormente explicado en el apartado tres.

\subsubsection{Código descartado}
Siendo Chcantabrico la primera web de la que se obtuvo los datos, sin gran conocimiento de Scrapy y sobre todo, sin saber realmente como sería la plataforma, el planteamiento de la obtención de datos se realizo de forma ajena a las Spider de Scrapy. Viendo que los datos no estaban presentes en la web y que Scrapy no dispone de interacción JavaScript por defecto, la única alternativa viable en esos momentos fue probar a realizar una llamada cURL por terminal, al ver que efectivamente mediante cURL era posible obtener los datos deseados, se escribió un script para los datos de nivel y otro para los de precipitación.

\begin{lstlisting}[language=Python, caption={Script de obtención de datos pluviometricos descartado}]
	import pandas as pd
	import io
	import requests
	import json
	
	with open('codigos_estaciones_chcantabrico.json', 'r', encoding='utf-8') as f:
		data = json.load(f)
		datos = []
		
		for item in data:

			params_pluvio = {
				'p_p_id': 'GraficaEstacion_INSTANCE_ND81Xo17PIZ7',
				'p_p_lifecycle': '2',
				'p_p_state': 'normal',
				'p_p_mode': 'view',
				'p_p_resource_id': 'downloadCsvPluvio',
				'p_p_cacheability': 'cacheLevelPage',
				'_GraficaEstacion_INSTANCE_ND81Xo17PIZ7_cod_estacion': f'{item["codigo"]}',
				'_GraficaEstacion_INSTANCE_ND81Xo17PIZ7_tipodato': 'pluvio',
			}
			
			response_pluvio = requests.get('https://www.chcantabrico.es/precipitacion-acumulada', params=params_pluvio)
			if response_pluvio.status_code == 200:
				if not response_pluvio.text.startswith('-'):
					urlData = response_pluvio.text
					rawData = pd.read_csv(io.StringIO(urlData), delimiter=';', encoding='utf-8', header=1)
					rawData.columns = ['fecha y hora', 'precipitacion (mm)']
					parsedData = rawData.to_json(orient="records")
					
					estacion = {
						'estacion': item["codigo"],
						'datos': json.loads(parsedData)
					}
					datos.append(estacion)
				else:
					print(f'{item["estacion"]} Error retrieving data: 404')
					print("-------------------")
			else:
				print(f'{item["estacion"]} Error retrieving data: {response_pluvio.status_code}')
				print("-------------------")
		
		with open('../../JSONs/RawData/datos_pluvio_chcantabrico.json', 'w', encoding='utf-8') as outfile:
			json.dump(datos, outfile)
\end{lstlisting}

Todo el apartado de tratamiento de los datos es idéntico al realizado con la Spider, solo que en vez de usar FormRequest, hacemos uso de la librería requests para realizar la llamada get(), tras realizarla, se comprueba que haya sido exitosa (esta comprobación la realiza Scrapy automáticamente) y, en caso de serlo se realiza todo el tratamiento.\newline
\newline
Aunque en esta versión se almacenan todos los datos en un mismo JSON, originalmente los datos eran guardados en un CSV por cada estación, de tal manera que el nombre del CSV era el mismo que el de la estación perteneciente. Más adelante al consolidar más la plataforma, sobre todo el uso de Django para la creación de una API, se vio que era más útil guardar los datos no solo en formato JSON si no que disponer de un único fichero por estación, de esta forma solo seria necesario realizar una única llamada por estación a la API para cargar los datos. Llegando a esta versión del script.\newline
\newline
Posteriormente, llegado el momento de la automatización quedo claro que, aun siendo posible automatizar el proceso con el script anterior, iba a suponer un problema para la modularidad del proyecto. El tener múltiples scripts de diferentes fuentes solo aumentaba la complejidad, necesitando de un script de ejecución por cada web la cual se deseaba trabajar con. Implicando la necesidad de no solo crear el código de extracción y tratamiento de datos, sino el de ejecución también. Esto rompía hasta cierto punto la experiencia deseada, el usuario solo debería preocuparse de los datos y la única interacción que debería realizar con el proceso de ejecución seria el añadir una única linea de comando referencioando el nombre usado para el nuevo modulo.\newline
\newline
Es por eso que se investigo la posibilidad de obtener los datos mediante Scrapy, estudiando los diferentes objetos Request proporcionados, hasta llegar a la versión actual con el uso de FormRequest.

\subsection{MeteoNavarra}

\begin{lstlisting}[language=Python, caption={MeteoNavarra Code Spider}]
	import scrapy
	
	
	class MeteonavarraCodeSpider(scrapy.Spider):
		name = "meteoNavarra_code_spider"
		allowed_domains = ["meteo.navarra.es"]
		start_urls = ["http://meteo.navarra.es/estaciones/mapadeestaciones.cfm#"]
		custom_settings = {
			'FEEDS': {
				'JSONs/RawCode/codigos_meteoNavarra.json': {
					'format': 'json',
					'encoding': 'utf-8',
					'overwrite': True,
				}
			}
		}
		
		def parse(self, response):
			rows = response.css('div#tabAUTO script::text').getall()
			
			for row in rows:
			yield {
				'estacion': row.split(',')[3],
				'codigo': row.split(',')[0].split('(')[1],
			}
\end{lstlisting}

Probablemente una de las Spider más simples entre todas las usadas, aunque como se comenta anteriormente, el mayor desafío fue la estructuración de la web y como obtener los datos de esta.

\begin{lstlisting}[language=Python, caption={MeteoNavarra Data Spider}]
	import datetime
	import json
	
	import scrapy
	
	
	class MeteonavarraDataSpider(scrapy.Spider):
		name = "meteoNavarra_data_spider"
		allowed_domains = ["meteo.navarra.es"]
		custom_settings = {
			'FEEDS': {
				'JSONs/RawData/datos_meteoNavarra.json': {
					'format': 'json',
					'encoding': 'utf-8',
					'overwrite': True,
				}
			}
		}
		
		def start_requests(self):
			current_date = datetime.date.today()
			delta = datetime.timedelta(days=1)
			tomorrow_date = current_date + delta
			yesterday_date = current_date - delta
			tomorrow_date_format = tomorrow_date.strftime(" %d%%2F %m%%2F%Y").replace(' 0', '')
			yesterday_date_format = yesterday_date.strftime(" %d%%2F %m%%2F%Y").replace(' 0', '')
			
			with open("JSONs/RawCode/codigos_meteoNavarra.json", encoding="utf-8") as f:
				data = json.load(f)
				for estacion in data:
				url = f'http://meteo.navarra.es/estaciones/estacion_datos_m.cfm?'
				'IDEstacion={estacion["codigo"]}&p_10' \
				f'=1&p_10=2&p_10=3&p_10=11&fecha_desde={yesterday_date_format}'
				'&fecha_hasta={tomorrow_date_format}'
				yield scrapy.Request(url, self.parse)
		
		def parse(self, response):
			rows = response.css('table.border tr:not([bgcolor*="#FFFFFF"])')
			estacion = response.css('table a::attr(href)')[7].get()
			
			datos = []
			for row in rows:
				dato = {
					'fecha y hora': row.xpath('./td[1]/text()').get().strip().replace(' ', ' ') + ':00',
					'temperatura (C)': row.xpath('./td[2]/font/text()').get(),
					'humedad relativa (%)': row.xpath('./td[3]/font/text()').get(),
					'radiacion global (W/m^2)': row.xpath('./td[4]/font/text()').get(),
					'precipitacion (l/mm^2)': row.xpath('./td[5]/font/text()').get(),
				}
				
				if dato['radiacion global (W/m^2)'] == '- -':
				dato['radiacion global (W/m^2)'] = None
				
				if dato['precipitacion (l/mm^2)'] != '- -':
				datos.append(dato)
			
			if datos:
				yield {
					'estacion': estacion.split('idestacion=')[1].split('&')[0],
					'datos': datos,
				}
			
			next_page = response.css("table a::attr(href)").getall()
			page_number = response.xpath("//b/text()").getall()
			if not page_number[0] == page_number[1]:
				next_page = response.urljoin(next_page[7])
				yield scrapy.Request(next_page, callback=self.parse)
\end{lstlisting}

A diferencia del resto de páginas, que te definen automáticamente una franja de fechas a mostrar, meteoNavarra da la posibilidad de que el usuario elija las fechas que desee ver, es por eso que hace uso de datetime, obteniendo el día de ayer y el de mañana (meteoNavara excluye los datos del día indicado), para su correcto funcionamiento, mediante el método strftime, formateamos el datetime para que corresponda con el codificado URL.\newline
\newline
Tras obtener los datos lo primero que se realiza es si existe un valor de radiación global, esto se hace debido a que no todas las estaciones proporcionan este datos, posteriormente, puesto que la web muestra todas las franjas horarias dentro del  que se corresponde con el día actual, se eliminan todas esas horas sin datos con la comprobación respecto a la precipitación. A continuación, en caso de que existan datos a guardar se realiza el yield.\newline
\newline
Finalmente, como los datos de ciertas estaciones estan repartidos en dos páginas, navegamos a esa segunda página para realizar el mismo proceso descrito anteriormente.\newline
\newline
Esto es posible gracias a que yield, aunque a groso modo funcione como un return, no fuerza el final de la ejecución, por lo que todo código por debajo de este sera ejecutado, llegando a haber múltiples yields en una misma función.

\begin{lstlisting}[language=Python, caption={MeteoNavarra Coordenates Spider}]
	import json
	
	import scrapy
	
	
	class MeteonavarraCoordSpider(scrapy.Spider):
		name = "meteoNavarra_coord_spider"
		allowed_domains = ["meteo.navarra.es"]
		custom_settings = {
			'FEEDS': {
				'JSONs/RawData/coordenadas_meteoNavarra.json': {
					'format': 'json',
					'encoding': 'utf-8',
					'overwrite': True,
				}
			}
		}
		
		def start_requests(self):
			with open("JSONs/RawCode/codigos_meteoNavarra.json", encoding="utf-8") as f:
				data = json.load(f)
				for estacion in data:
				url = f'http://meteo.navarra.es/estaciones/estacion.cfm?'
				'IDestacion={estacion["codigo"]}'
				yield scrapy.Request(url, self.parse)
		
		def parse(self, response):
			coordenadas = response.css('td::text')[19].get()
			estacion = response.css('input::attr(value)').get()
			
			yield {
				'coordenadas': coordenadas.strip().replace('\r\n\t\t', ' ').replace(' (*)', ''),
				'estacion': estacion,
			}
\end{lstlisting}

Por último, necesitamos de una Spider para la obtención de las coordenadas pues no están disponibles para obtener dentro de las páginas anteriores, pero al igual que la de código no dispone realmente de nada relevante que mencionar.

\subsection{Agua en Navarra}
Las Spiders de Agua en Navarra son las más complejas entre todas, usando una filosofía ligeramente distinta al resto de Spiders y por el uso de Selemiun para poder interactuar con la web mediante JavaScipt.

\begin{lstlisting}[language=Python, caption={Agua en Navarra Code Spider}]
	import scrapy
	
	
	class AguaEnNavarraCodeSpider(scrapy.Spider):
		name = "aguaEnNavarra_code_spider"
		allowed_domains = ["administracionelectronica.navarra.es"]
		start_urls = ["https://administracionelectronica.navarra.es/aguaEnNavarra/"
		"ctaMapa.aspx?IDOrigenDatos=1&IDMapa=1"]
		custom_settings = {
			'FEEDS': {
				'JSONs/RawCode/codigos_aguaEnNavarra.json': {
					'format': 'json',
					'encoding': 'utf-8',
					'overwrite': True,
				}
			}
		}
		
		def parse(self, response):
			for link in response.css('dl#navarramap a::attr(href)'):
				if link.get() != 'ctaMapa.aspx?IdMapa=1&IDOrigenDatos=1':
					yield response.follow(link.get(), callback=self.parse_area)
		
		def parse_area(self, response):
			for link in response.css('area[shape="rect"]::attr(href)'):
				yield response.follow(link.get(), callback=self.parse_data)
		
		def parse_estacion(self, response):
			urls = response.xpath('//span/a/@href').getall()
			estacion = response.css('div#bloq_iconos span span::text').getall()
			codigoEstacion = response.css("form#frmDatosEstacion::attr(action)").get()
			codigos = []
			for url in urls:
				codigos.append(url.split('=')[1])
			
			yield {
				'descripcion': estacion[0],
				'municipio': estacion[1],
				'rio': estacion[2],
				'coordenadas': estacion[3],
				'estacion': codigoEstacion.split('=')[-1],
				'codigos': codigos,
			}
\end{lstlisting}

Como se puede apreciar, esta Spider hace uso de tres funciones parse(), al contrario del resto que solo disponían de una. Esto se debe hasta cierto punto a la estructuración de la página, haciendo más sencillo navegar por ella que crear una Spider por cada página que tenemos que visitar.\newline
\newline
Inspeccionando el código veremos que partimos de una start\_urls, llamando con ella a parse(), de esa web nos interesan recorrer todos los link menos aquel en el que ya estamos ('ctaMapa.aspx?IdMapa=1\&IDOrigenDatos=1'), llevándonos a la página de cada area de estaciones. El uso de response.follow() es equivalente a realizar un Request, pero no necesita pararle una URL completa, solo con la ruta es capaz de generar la llamada, en caso de necesitar explicitamente Request esta seria la manera.
\begin{verbatim}
	yield Request(url=response.urljoin(link.get()), callback=self.parse_area)
\end{verbatim}
Dentro de callback indicamos a que función debe realizar la llamada, pasando de parse() a parse\_area(). En esta realizamos el mismo proceso, obtenemos todos los link a las webs, ya sí, de la estación y llamamos a parse\_estacion().\newline
\newline
Una vez en parse\_estacion() como ocurre con Chcantabrico, la estación dispone de múltiples códigos, aquel que referencia la estación, uno para el nivel y aunque no en todas, otro para el caudal del río.

\begin{lstlisting}[language=Python, caption={Agua en Navarra Data Spider}]
	import scrapy
	from scrapy_selenium import SeleniumRequest
	from selenium.webdriver.common.by import By
	from selenium.webdriver.support import expected_conditions as EC
	
	
	class AguaEnNavarraDataSpider(scrapy.Spider):
		name = "aguaEnNavarra_data_spider"
		allowed_domains = ["administracionelectronica.navarra.es"]
		start_urls = ["https://administracionelectronica.navarra.es/aguaEnNavarra/"
		"ctaMapa.aspx?IDOrigenDatos=1&IDMapa=1"]
		custom_settings = {
			'FEEDS': {
				'JSONs/RawData/datos_aguaEnNavarra.json': {
					'format': 'json',
					'encoding': 'utf-8',
					'overwrite': True,
				}
			}
		}
		
		def parse(self, response):
			for link in response.css('dl#navarramap a::attr(href)'):
				if link.get() != 'ctaMapa.aspx?IdMapa=1&IDOrigenDatos=1':
					yield response.follow(link.get(), callback=self.parse_area)
			
		def parse_area(self, response):
			for link in response.css('area[shape="rect"]::attr(href)'):
				yield response.follow(link.get(), callback=self.parse_estacion)
			
		def parse_estacion(self, response):
			for link in response.css('div#divResultadosAforo a::attr(href)'):
				yield SeleniumRequest(
				url=response.urljoin(link.get()),
				wait_time=2,
				wait_until=EC.element_to_be_clickable((By.ID, 'btnDatosNumericos')),
				callback=self.parse_data,
				script='document.querySelector("#btnDatosNumericos").click()',
				)
			
		def parse_data(self, response):
			tipo = response.css('span#lblSenal::text').get()
			estacion = response.css('li#cabecera_nombreEstacion a::attr(href)').get()
			fechas = response.css('span.cont_fecha_gra::text').getall()
			valores = response.css('span.cont_valor_gra::text').getall()
			
			datos = []
			if tipo == "Nivel Rio":
				for i, fecha in enumerate(fechas):
					dato = {
						'fecha y hora': fecha.strip() + ':00',
						'nivel (m)': valores[i].strip(),
					}
					datos.append(dato)
			else:
				for i, fecha in enumerate(fechas):
					dato = {
						'fecha y hora': fecha.strip() + ':00',
						'caudal (m^3/s)': valores[i].strip(),
					}
					datos.append(dato)
			
			yield {
				'estacion': estacion.split('=')[-1],
				'datos': datos,
			}
\end{lstlisting}

Esta Spider se puede decir que es una amplifican de la anterior, pues parte de la misma URL de inicio y va recorriendo las mismas webs que la anterior, solo que esta recorre una página más. De esta forma a diferencia del resto de Spider destinadas a obtener datos, no necesita de un fichero con los códigos para funcionar.\newline
\newline
Lo primero que llama la atención de esta Spider respecto al resto es el uso de SeleniumRequest, este tipo de objeto Request pertenece a la librería scrapy\_selenium, siendo una alternativa sencilla de unificar las funcionalidades de Selenium en Scrapy, de esta forma, podemos lidiar con la falta de integración de interacción con JavaScript en Scrapy.\newline
\newline
Tras realizar el mismo proceso que antes, la Spider llega a la función parse\_estacion(), esta vez, en vez de obtener los datos de la estación, accedemos a los links que permiten mostrar los datos mediante el SeleniumRequest. Como el link al que somo dirigidos muestra los datos en forma de diagrama y, para acceder a los datos numéricos es necesario pulsar un botón, SeleniumRequest nos permite, esperar a que este esté correctamente cargado con el argumento wait\_until, indicando que elemento deseas esperar a ser cargado; en caso de que no se cargue, es recomendable usar el argumento wait\_time, que usado junto al anterior, establece el tiempo antes de dar la llamada por errónea; si el botón se carga correctamente, en el argumento script indicamos que se debe pulsar sobre este, llevándonos finalmente a los datos numéricos.\newline
\newline
En la funcion parse\_data(), obtenemos los datos por columna en vez de fila, pues la web no hace un correcto uso formateo por tabla, pues no existe esta tabla, insertando los datos sobre un elemento div y formateandolos posteriormente con CSS. Esta forma de obtener los datos fuerza la obtención de las fechas por un lado y los valores por otro, resultando en dos listas que debemos unir. A su vez, ya que hacemos uso de una Spider para obtener todos los datos aunque estén presentes en distintas páginas, hace necesaria la comprobación inicial de que datos estamos recibiendo para poder almacenarlos correctamente.

\subsubsection{Configuración Selenium}
Para que SeleniumRequest funcione, es necesario incluir las siguientes lineas de código en el archivo settings.py generado por Scrapy.

\begin{lstlisting}[language=Python, caption={Agua en Navarra configuración Selenium}]
	from shutil import which
	
	SELENIUM_DRIVER_NAME = 'chrome'
	SELENIUM_DRIVER_EXECUTABLE_PATH = which('chromedriver')
	SELENIUM_DRIVER_ARGUMENTS = ['--headless',
	'--disable-logging',
	'--disable-in-process-stack-traces',
	'--log-level=1',
	'--disable-extensions'
	]
	
	DOWNLOADER_MIDDLEWARES = {
		'scrapy_selenium.SeleniumMiddleware': 800
	}
\end{lstlisting}

En nuestro caso indicamos que el navegador deseado para realizar el trabajo es Chrome e indicamos mediante el método which la ruta del ejecutable del driver de chrome.\newline
\newline
Los argumentos usados indican, --headless significa que las instancias de Chrome creadas sean sin interfaz, de esta forma no tendremos cientos de ventanas de Chrome abriéndose y cerrándose cada vez que se ejecute la Spider; --disable-logging como su nombre indica elimina el Log; --disable-in-process-stack-traces desactiva el stack-trace, esto se realiza unicamente con el fin de intentar mejorar el rendimiento; --log-level indica el tipo de log que desas mostrar, siendo INFO = 0, ALERTA = 1, ERROR = 2 y FATAL = 3, aunque el Log este desactivado esta bien indicar que clase de Log deseas que aparezca en caso de que se vuelva a activar; --disable-extensions desactiva todas las extensiones que podamos tener instaladas, nuevamente con el fin de mejorar el rendimiento.\newline
\newline
Una vez incluido el middleware, ya solo quedaria descargar el driver de chrome.\newline
\newline
Chromedriver.exe puede ser obtenido en la web \url{https://chromedriver.chromium.org/downloads}, descargando aquel que corresponda a la versión de Chrome instalada. Una vez descomprimido debe ser incluido dentro de la carpeta de proyecto de Scrapy.

\begin{figure} [h!]
	\centering
	\includegraphics[width=0.4\textwidth]{fig/chromedriver.png}
	\caption[Ruta chromedriver.exe]{Ruta chromedriver.exe}
	\label{fig:ej15}
\end{figure}

En caso de estar en Windows con esto ya es suficiente, aunque al usar Debian son necesarios unos paso más para configurar chromedriver.

\subsubsection{Alternativas probadas}
Antes de saber que es necesario dirigirse a la pagina donde se muestra la gráfica antes de poder acceder a la página con los datos numéricos, se probo con una Spider que accedía directamente a esta. Visto que el resultado obtenido estaba vacío y, que en apariencia la Spider era correcta, obteniendo teóricamente aquellos datos deseados, se comprobó la web manualmente, resultando en la obtención de un error \ref{fig:ej5}.\newline
\newline
Es por eso que, partiendo de un uso de Scrapy sin uso de extensiones de terceros, se probó una solución al problema siguiendo la metodología de recorrer las webs, de esta manera, visitaba inicialmente la web con la gráfica para posteriormente redirigirse a los datos numéricos. Resultando en un nuevo fracaso. Esta vez si que se obtenían datos, aunque no de forma correcta, muchas estaciones aparecían repetidas múltiples veces, de tres a siete veces en el peor de los casos visto. Siendo el resultado esperado la aparición de una misma estación un total de dos veces, una con los datos del caudal y otra por los datos del nivel. Volviendo a la comprobación manual resulto en el mismo comportamiento, por alguna razón, una vez dentro de la web de los datos numéricos, al cambiar el código de la estación y recargar la pagina, lo mismo se actualizaban los datos para mostrar los de la nueva estación como no lo hacían, mostrando los datos de la anterior.\newline
\newline
Esto causo la incertidumbre de si era posible obtener datos la web de forma fiable. Finalmente, barajando la posibilidad de pulsar el botón para acceder los datos, se dio con la herramienta usada en la versión final, Selenium y, tiempo después con una alternativa más moderna a esta, Playwright.\newline
\newline
Por suerte, Selenium, siendo usado mediante la extensión scrapy\_selenium, resulto funcionar a la primera, aunque a un costo temporal relativamente alto, en comparación con las demás Spider, con una media de dos minutos y medio de ejecución.\newline
\newline
Al ser código destinado a ser ejecutado en intervalos de quince minutos, un tiempo de ejecución así no debería acarrear ningún problema, pero aun y todo, se probo una cuarta alternativa mediante Playwright, prometiendo mejoras en los tiempos de ejecución y una mejor optimización.\newline
\newline
Antes que nada, cabe mencionar que Playwright no es compatible con Windows y, tampoco lo es con Debian 11 en su versión actual, resultando ser Debian 11 la versión usada para esta proyecto, por lo que no se ha podido llegar a comprobar el funcionamiento del siguiente código, aunque debería ser correcto.
