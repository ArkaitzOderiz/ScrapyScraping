\chapter[Conclusiones y Trabajo Futuro]{Conclusiones y Trabajo Futuro}
\label{Chap6}

El proyecto a sido llevado a cabo finalmente con las siguientes herramientas, PostGreSQL el la base de datos, Django para la API, Scrapy y Selenium en la plataforma de adquisición de datos y Cron para la automatización de las tareas. Con esto, se ha podido realizar una plataforma, que ejecuta scripts de forma regular en intervalos de tiempo predefinidos para cada página usada, con el fin de no generar más trafico web del realmente necesario.\newline
\newline
En este proyecto se ha pretendido crear una arquitectura lo más modular posible, ya sea por el uso de múltiples máquinas virtuales, como por el uso de un entorno virtual por cada uno de los distintos apartados que la forman e, incluso en la forma de codificarla. Intenta ser lo más intuitiva posible a la hora de poder realizar cambios sobre esta, diferenciando claramente cada elemento.\newline
\newline
El mayor inconveniente de una plataforma así es la redundancia de código, al ser múltiples entornos independientes es necesario que mucho código sea repetido en cada uno de ellos, cosa que si fuera un único entorno en el que se ejecutara todo, con una clase sobre la que heredar y o una interfaz, seria mucho el código que nos ahorraríamos. Aunque se perdería gran parte de la flexibilidad proporcionada.\newline
\newline
Django no cabe duda de ser un Framework completo sobre el que trabajar, pero a resultado ser en cierta mediada más un problema que una solución, su sistema de gestión de versiones sobre los cambios realizados en la base de datos, supuso tener que rediseñar la arquitectura a una versión más compleja. A su vez, al crear las tablas de la base de datos a partir de los modelos de Django, hizo que no se pudiera hacer uso de una clave primaria compuesta, ya que Django no da soporte a tablas así, siendo una de las peticiones que más ha realizado la comunidad de Django.\newline
\newline
A nivel de obtención de datos, se han presentado múltiples problemas sobre las distintas webs. Como se ha visto, algunas no hacen un uso correcto de HTML, creando tablas a partir de CSS sobre elementos \textit{div} en vez de usar las tablas definidas en HTML; o no hacen un buen uso de los atributos \textit{id} y \textit{class}, haciendo la obtención de datos algo tedioso, llegando a tener que filtrar datos mediante los atributos CSS, siendo un claro ejemplo la plataforma de El Agua en Navarra, donde para obtener los códigos de la estación, son filtrados aquellos elementos con el atributo CSS \textit{shape=''rect``}.\newline
\newline
Otros problemas encontrados están relacionados con las coordenadas, pues el que CHCántabrico no disponga de las coordenadas de las estaciones presentes, siendo un dato que debería ser de fácil acceso público, casi hace que no pueda ser usada y, no ha sido así gracias a que se lograron las coordenadas de un segunda página. Y eso que no se ha llegado a disponer de todas las coordenadas. El segundo problema radica en que todas las estaciones hacen uso de un estándar distinto a la hora de mostrar las coordenadas, por lo que habría que crear un Script capaz de convertir las coordenadas a un mismo estándar para poder ser usadas correctamente.\newline
\newline
Centrado en técnicas de scraping web, este proyecto esta limitado por los datos ofrecidos de forma abierta en la web, siendo su punto débil, aquel del cual no disponemos control alguno. A día de hoy, la plataforma dispone de cuatro fuentes distintas de las cuales obtener datos, pero esta comprometida a que se mantengan invariables en el tiempo. Es por esto, que en un proyecto así necesariamente se debe valorar la búsqueda de datos de forma activa. No solo para su mantenimiento a largo plazo, sino como forma de extender su alcance.\newline
\newline